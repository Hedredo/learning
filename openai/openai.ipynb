{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```pip install openai```\n",
    "\n",
    "Get your key on https://platform.openai.com/api-keys\n",
    "\n",
    "After you need to instantiate a OpenAI object to load the client used to reach endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the key\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Create a client\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(content: str, model: str = \"gpt-4o-mini\", temperature: int = 2, max_tokens=100):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints\n",
    "\n",
    "OpenAi API has differents endpoints to make requests on differents services. Here's some of theses endpoints :\n",
    "- `chatbot` openai.chat.completions.create\n",
    "    - Question/Answering\n",
    "    - Find & Replace\n",
    "    - Text Summarization\n",
    "    - Content generation\n",
    "    - Sentiment analysis & Classification\n",
    "    - Translation\n",
    "- `moderation` openai.moderations.create\n",
    "- `audio transcript` openai.audio.transcriptions.create\n",
    "- `audio translation` openai.audio.translations.create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat.completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles\n",
    "\n",
    "There is 3 main roles:\n",
    "- system : Controls the assistant behaviour\n",
    "- user : Prompt sent to the assistant by the user\n",
    "- assistant : Chat completion by the assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistant behaviour (system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a request to the Chat Completions endpoint\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  max_tokens=150,\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful data science tutor.\"},\n",
    "    {\"role\": \"user\",\n",
    "    \"content\": \"What is the difference between a for loop and a while loop?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Extract the assistant's text response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In context learning / RAG\n",
    "\n",
    "In context learning can helps guide the model to answer.<br>\n",
    "The **crucial part** is to add to the system prompt a clear instruction as : `Answer following the style used in the previous example(s).`<br>\n",
    "This way, the chat will look the previous part of conversation including an example.<br>\n",
    "Show him how :\n",
    "- To structure its answer\n",
    "- The response style\n",
    "- You can also add advance double turn to get a first response and then add another set of instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `type()` function in Python is a built-in function that is used to determine the type of an object. When you provide an object as an argument to `type()`, it returns the type of that object. This can be useful for debugging or understanding what kind of data you are working with.\n",
      "\n",
      "Here's how you can use it:\n",
      "\n",
      "1. **Basic Usage**:\n",
      "   ```python\n",
      "   x = 5\n",
      "   print(type(x))  # Output: <class 'int'>\n",
      "   ```\n",
      "\n",
      "2. **With Different Data Types**:\n",
      "   ```python\n",
      "   y = 3.14\n",
      "   print(type(y))  # Output: <class 'float'>\n",
      "\n",
      "   z = \"Hello, World!\"\n",
      "   print(type(z))  # Output: <class 'str'>\n",
      "\n",
      "   my_list = [1, 2, 3]\n",
      "   print(type(my_list))  # Output: <class 'list'>\n",
      "   ```\n",
      "\n",
      "3. **With Custom Classes**:\n",
      "   If you define a custom class, the `type()` function will return the class you defined:\n",
      "   ```python\n",
      "   class MyClass:\n",
      "       pass\n",
      "\n",
      "   obj = MyClass()\n",
      "   print(type(obj))  # Output: <class '__main__.MyClass'>\n",
      "   ```\n",
      "\n",
      "4. **Checking if Two Types are Equal**:\n",
      "   You can also use `type()` in conditional expressions:\n",
      "   ```python\n",
      "   if type(x) == int:\n",
      "       print(\"x is an integer.\")\n",
      "   ```\n",
      "\n",
      "In addition to simply checking the type, `type()` can also be used to create new types dynamically and is part of Python's metaprogramming capabilities, but that is a more advanced topic.\n",
      "\n",
      "Overall, `type()` is an essential function that helps you understand the nature of the objects you are working with in Python.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "   model=\"gpt-4o-mini\",\n",
    "   # Add a user and assistant message for in-context learning\n",
    "   messages=[\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n",
    "   ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Purpose**: The `type()` function is used to determine the type of an object in Python.\n",
      "\n",
      "2. **Input**: It takes a single argument, which can be any Python object (like a number, string, list, etc.).\n",
      "\n",
      "3. **Output**: It returns the type of the provided object as a type object. For example, `type(5)` will return `<class 'int'>`, indicating that the object is an integer. \n",
      "\n",
      "4. **Usage**: This function is useful for debugging and understanding the nature of variables in your code.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "   model=\"gpt-4o-mini\",\n",
    "   # Add a user and assistant message for in-context learning\n",
    "   messages=[\n",
    "     {\"role\": \"system\", \"content\": \"You are a Python tutor. Answer following the style used in the previous example.\"}, # This Part is CRUCIAL\n",
    "     {\"role\": \"user\", \"content\": \"Explain me what the int() function does\"},\n",
    "     {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "     1. Input : The int() take as an input an object and try to evaluate it as an integer.\n",
    "     2. Output : By example, int('5'), will return successfully 5.\n",
    "     \"\"\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n",
    "   ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Explain what pi is.\n",
      "Assistant:  Pi (π) is a mathematical constant that represents the ratio of a circle's circumference to its diameter. This means that for any circle, if you divide the circumference (the distance around the circle) by the diameter (the distance across the circle through its center), you will always get the same value, which is approximately 3.14159.\n",
      "\n",
      "Here are some key points about pi:\n",
      "\n",
      "1. **Irrational Number**: Pi is an irrational number, which means it cannot be expressed as \n",
      "\n",
      "User:  Summarize this in two bullet points.\n",
      "Assistant:  - Pi (π) is the ratio of a circle's circumference to its diameter, approximately equal to 3.14159.\n",
      "- It is an irrational number, meaning it cannot be expressed as a simple fraction and its decimal representation goes on forever without repeating. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    \n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}\n",
    "    messages.append(user_dict)\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant: \", response.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is like a box of chocolates\" is a famous quote from the movie *Forrest Gump*. It suggests that life is full of surprises and unpredictability, much like a box of assorted chocolates where you never know what you're going to get. Each experience can be sweet or bittersweet, and it emphasizes the importance of embracing whatever comes your way. How do you interpret this idea?\n",
      "\"Life is like a box of chocolates; you never know what you're gonna get.\" This famous quote from the movie *Forrest Gump* suggests that life is full of surprises and unpredictable moments. Just as each chocolate has a different filling, each experience in life can bring both joy and challenge. It captures the essence of embracing uncertainty and being open to whatever comes your way. What are your thoughts on this metaphor?\n"
     ]
    }
   ],
   "source": [
    "# Most likely complete the prompt if you don't specify any instructions depending of the temperature parameter to add verbose\n",
    "content = get_response(content=\"Life is like a box of chocolates.\", temperature=5)\n",
    "print(content)\n",
    "content = get_response(content=\"Life is like a box of chocolates.\", temperature=0)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B9RjXbnvGqgM4nIkTth1P2XmQ8CHZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The OpenAI Python module provides a set of endpoints that allow you to interact with OpenAI\\'s API services, including text generation, image generation, and other functionalities. Here’s a basic overview of how the endpoints work in the OpenAI Python module:\\n\\n### Installation\\n\\nFirst, ensure you have the OpenAI Python package installed. You can install it using pip:\\n\\n```bash\\npip install openai\\n```\\n\\n### Setting Up\\n\\nTo use the OpenAI module, you need to set up your API key. You can do this by setting it directly in your code or by using environment variables.\\n\\n```python\\nimport openai\\n\\n# Set your OpenAI API key\\nopenai.api_key = \\'your-api-key-here\\'\\n```\\n\\n### Common Endpoints\\n\\n1. **Text Completion** (e.g., using the GPT model)\\n\\n   You can generate text with the `Completion.create()` method. Here’s how it typically works:\\n\\n   ```python\\n   response = openai.Completion.create(\\n       model=\"text-davinci-003\",  # Model name\\n       prompt=\"What is the capital of France?\",  # Your input prompt\\n       max_tokens=50  # Maximum number of tokens to generate\\n   )\\n\\n   print(response.choices[0].text.strip())\\n   ```\\n\\n   The response will contain multiple choices, and you can access the generated text through `response.choices`.\\n\\n2. **Chat Completion** (e.g., using the ChatGPT model)\\n\\n   For chat-based interactions, you can use the `ChatCompletion.create()` method:\\n\\n   ```python\\n   response = openai.ChatCompletion.create(\\n       model=\"gpt-3.5-turbo\",  # Chat model name\\n       messages=[\\n           {\"role\": \"user\", \"content\": \"Hello, how are you?\"}  # User message\\n       ]\\n   )\\n\\n   print(response[\\'choices\\'][0][\\'message\\'][\\'content\\'].strip())\\n   ```\\n\\n3. **Image Generation** (e.g., using DALL-E)\\n\\n   To generate images, you would use the `Image.create()` method:\\n\\n   ```python\\n   response = openai.Image.create(\\n       prompt=\"A beautiful sunrise over a mountain range\",\\n       n=1,  # Number of images to generate\\n       size=\"512x512\"  # Dimension of the image\\n   )\\n\\n   image_url = response[\\'data\\'][0][\\'url\\']\\n   print(image_url)\\n   ```\\n\\n4. **Fine-tuning Models**\\n\\n   The OpenAI API also allows for fine-tuning of models, though this information is typically accessed through different methods specific for handling training data and managing fine-tuned models.\\n\\n### Handling Responses\\n\\nThe responses from OpenAI API calls are generally structured JSON objects, which you can navigate like a dictionary to extract the information you need (such as generated text, URLs for images, etc.).\\n\\n### Error Handling\\n\\nYou should also implement error handling while making requests. This usually involves checking for `openai.error.OpenAIError` to handle various exceptions.\\n\\n```python\\ntry:\\n    response = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Hello\")\\nexcept openai.error.OpenAIError as e:\\n    print(f\"An error occurred: {e}\")\\n```\\n\\n### Conclusion\\n\\nThe endpoints provided by the OpenAI Python module offer powerful capabilities for generating text, creating images, and more. Always refer to the [official documentation](https://platform.openai.com/docs) for the latest updates and detailed examples for each endpoint.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741591259, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=715, prompt_tokens=18, total_tokens=733, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How works the endpoints in openai module for python?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to browse the internet or access websites, including the OpenAI documentation, as my knowledge was last updated in October 2021. However, I can provide you with a general overview of how endpoints typically work in the OpenAI Python module based on the information available up to that time.\n",
      "\n",
      "The OpenAI Python module allows you to interact with various AI models, including GPT-3, by making requests to different endpoints. Here’s how it generally works:\n",
      "\n",
      "1. **Installation**: You start by installing the OpenAI Python package using pip:\n",
      "   ```bash\n",
      "   pip install openai\n",
      "   ```\n",
      "\n",
      "2. **Authentication**: You need to set your API key to authorize requests. You can do this by setting the `OPENAI_API_KEY` environment variable or directly in your script:\n",
      "   ```python\n",
      "   import openai\n",
      "   openai.api_key = 'your-api-key'\n",
      "   ```\n",
      "\n",
      "3. **Endpoints**:\n",
      "   - **Completion endpoint**: This is used to generate text completions based on a prompt. You can call it like so:\n",
      "     ```python\n",
      "     response = openai.Completion.create(\n",
      "         engine=\"text-davinci-003\", # or other models like text-curie-001, text-babbage-001, etc.\n",
      "         prompt=\"Once upon a time\",\n",
      "         max_tokens=50\n",
      "     )\n",
      "     print(response.choices[0].text.strip())\n",
      "     ```\n",
      "\n",
      "   - **Chat endpoint**: For chat-based interactions, you might use the chat endpoint (for models like GPT-4), structured to handle a conversation:\n",
      "     ```python\n",
      "     response = openai.ChatCompletion.create(\n",
      "         model=\"gpt-4\",\n",
      "         messages=[\n",
      "             {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
      "         ]\n",
      "     )\n",
      "     print(response.choices[0].message['content'])\n",
      "     ```\n",
      "\n",
      "   - **Other endpoints**: Depending on the version of the API, there might be additional endpoints for image generation (e.g., DALL-E), file operations, or embeddings. Make sure to refer to the latest documentation for the details.\n",
      "\n",
      "4. **Parameters**: Each endpoint typically accepts various parameters that modify its behavior, such as:\n",
      "   - `max_tokens`: Maximum number of tokens to generate.\n",
      "   - `temperature`: Controls randomness (higher means more random).\n",
      "   - `top_p`: Controls diversity via nucleus sampling.\n",
      "\n",
      "5. **Error Handling**: You should include error handling (`try`/`except`) to deal with potential exceptions such as reaching API limits or invalid inputs.\n",
      "\n",
      "6. **Rate Limits and Best Practices**: Respect the rate limits specified by OpenAI to avoid being throttled and follow best practices for efficient use of the API.\n",
      "\n",
      "To get the most accurate and up-to-date information about specific endpoints and their usage, please visit the [OpenAI API documentation](https://platform.openai.com/docs) directly.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find & Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A plane is a vehicle that is typically powered by jet engines or propellers. It has wings and is designed to carry passengers and/or cargo through the air. Planes have become a ubiquitous part of modern society and are used for a wide variety of purposes, such as commuting, travel, and transportation of goods. Planes are often associated with freedom, independence, and mobility.\n"
     ]
    }
   ],
   "source": [
    "# Find & replace task\n",
    "prompt=\"\"\"\n",
    "Replace car with plane and adjust phrase in the following text:\n",
    "\n",
    "A car is a vehicle that is typically powered by an internal combustion engine or an electric motor.\n",
    "It has four wheels, and is designed to carry passengers and/or cargo on roads or highways.\n",
    "Cars have become a ubiquitous part of modern society, and are used for a wide variety of purposes, such as commuting, travel, and transportation of goods.\n",
    "Cars are often associated with freedom, independence, and mobility.\n",
    "\"\"\"\n",
    "\n",
    "content = get_response(content=prompt, temperature=2, max_tokens=100)\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Investment involves committing capital to various options—such as stocks, bonds, and real estate—with the aim of generating profit, requiring careful analysis of risks and rewards.  \n",
      "- Effective investment strategies, including diversification, can enhance long-term returns and financial security while minimizing the risk of losses.\n"
     ]
    }
   ],
   "source": [
    "# Summarize\n",
    "prompt=\"\"\"\n",
    "Summarize the following text into two concise bullet points:\n",
    "\n",
    "Investment refers to the act of committing money or capital to an enterprise with the expectation of obtaining an added income or profit in return.\n",
    "There are a variety of investment options available, including stocks, bonds, mutual funds, real estate, precious metals, and currencies.\n",
    "Making an investment decision requires careful analysis, assessment of risk, and evaluation of potential rewards.\n",
    "Good investments have the ability to produce high returns over the long term while minimizing risk.\n",
    "Diversification of investment portfolios reduces risk exposure.\n",
    "Investment can be a valuable tool for building wealth, generating income, and achieving financial security.\n",
    "It is important to be diligent and informed when investing to avoid losses.\n",
    "\"\"\"\n",
    "\n",
    "content = get_response(content=prompt, temperature=2, max_tokens=100)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Authentic Italian Flavors, Right in the Heart of Paris!\"\n"
     ]
    }
   ],
   "source": [
    "# Content generation. Increase temperature if you want more diversity\n",
    "prompt=\"Do create a slogan for a new italian restaurant in Paris city\"\n",
    "\n",
    "content = get_response(content=prompt, temperature=10, max_tokens=20)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Assigning a label to information provided into the promt\n",
    "- Sentiment analysis (binary, ordinal, etc...)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Positive\n",
      "2. Negative\n",
      "3. Neutral\n",
      "4. Positive\n"
     ]
    }
   ],
   "source": [
    "# Define a multi-line prompt to classify sentiment\n",
    "prompt = \"\"\"Classify the following statement as either negative, neutral or positive sentiment:\n",
    "1. Unbelievably good!\n",
    "2. Shoes fell apart on the second use.\n",
    "3. The shoes look nice, but they aren't very comfortable.\n",
    "4. Can't wait to show them off!\"\"\"\n",
    "\n",
    "# Create a request to the Chat Completions endpoint\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the classification of the companies into the specified categories:\n",
      "\n",
      "**Tech:**\n",
      "- Apple\n",
      "- Microsoft\n",
      "- Alphabet\n",
      "- Amazon\n",
      "- NVIDIA\n",
      "- Meta\n",
      "\n",
      "**Energy:**\n",
      "- Saudi Aramco\n",
      "- Tesla (also classified under Tech due to its innovation in technology and electric vehicles)\n",
      "\n",
      "**Luxury Goods:**\n",
      "- LVMH\n",
      "\n",
      "**Investment:**\n",
      "- Berkshire Hathaway\n",
      "\n",
      "Let me know if you need any further information!\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt for the categorization\n",
    "prompt = \"Classify the following companies in four categories as Tech, Energy, Luxury Goods and Investment: Apple, Microsoft, Saudi Aramco, Alphabet, Amazon, Berkshire Hathaway, NVIDIA, Meta, Tesla, LVMH\"\n",
    "\n",
    "# Create a request to the Chat Completions endpoint\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  max_tokens=100,\n",
    "  temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moderations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CategoryScores(harassment=5.243551186140394e-06, harassment_threatening=1.1516095810293336e-06, hate=4.767837526742369e-05, hate_threatening=3.2021056028952444e-08, illicit=None, illicit_violent=None, self_harm=9.466615438213921e-07, self_harm_instructions=5.426785065765216e-08, self_harm_intent=1.5536235764557205e-07, sexual=3.545879735611379e-06, sexual_minors=1.1304399549771915e-06, violence=0.0001064608441083692, violence_graphic=1.086988686438417e-05, self-harm=9.466615438213921e-07, sexual/minors=1.1304399549771915e-06, hate/threatening=3.2021056028952444e-08, violence/graphic=1.086988686438417e-05, self-harm/intent=1.5536235764557205e-07, self-harm/instructions=5.426785065765216e-08, harassment/threatening=1.1516095810293336e-06)\n"
     ]
    }
   ],
   "source": [
    "# Create a request to the Moderation endpoint\n",
    "response = client.moderations.create(\n",
    "    model=\"text-moderation-latest\",\n",
    "    input=\"My favorite book is To Kill a Mockingbird.\"\n",
    ")\n",
    "\n",
    "# Print the category scores\n",
    "print(response.results[0].category_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### audio.transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, o meu nome é Eduardo, sou CTO no Datacamp. Espero que esteja a gostar deste curso que o James e eu criamos para você. Esta API permite enviar um áudio e trazer para inglês. O áudio original está em português.\n"
     ]
    }
   ],
   "source": [
    "# Open the audio.m4a file\n",
    "audio_file = open(\"audio-portuguese.m4a\", \"rb\")\n",
    "\n",
    "# Create a transcript from the audio file\n",
    "response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### audio.translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Eduardo, I am a CTO at Datacamp. I hope you are enjoying this course that James and I have created for you. This API allows you to send an audio and bring it to English. The original audio is in Portuguese.\n"
     ]
    }
   ],
   "source": [
    "# Open the audio.m4a file\n",
    "audio_file = open(\"audio-portuguese.m4a\", \"rb\")\n",
    "\n",
    "# Write an appropriate prompt to help the model\n",
    "prompt = \"The audio file relates to a recent Bank World report\"\n",
    "\n",
    "# Create a translation from the audio file\n",
    "response = client.audio.translations.create(model=\"whisper-1\",\n",
    "                                            file=audio_file,\n",
    "                                            prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt styles\n",
    "\n",
    "- Zero Shot prompting : Don't provide any example\n",
    "- One Shot prompting : One example provided\n",
    "- Few Shot promptint : Many examples provides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
