{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "- Improve the parsing errors in the code\n",
    "- Check the consistency of the output compared to the web source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import inspect\n",
    "from html2text import HTML2Text\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple code from chatgpt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://docs.python.org/3/\"\n",
    "START_PAGE = \"howto/logging.html\"\n",
    "\n",
    "def get_links_from_index(start_url):\n",
    "    \"\"\"Récupère tous les liens internes pointant vers les pages de la doc.\"\"\"\n",
    "    response = requests.get(start_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"howto/\") and href.endswith(\".html\"):  # Filtrer les pages utiles\n",
    "            full_url = BASE_URL + href\n",
    "            links.append(full_url)\n",
    "    \n",
    "    return list(set(links))  # Supprimer les doublons\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrape une page et extrait le contenu utile.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"Untitled\"\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in soup.find_all(\"p\"))  # Extraire le texte des <p>\n",
    "    \n",
    "    return f\"# {title}\\n\\n{content}\\n\\n---\\n\"\n",
    "\n",
    "def save_to_file(content, filename=\"docs_python.md\"):\n",
    "    \"\"\"Enregistre le texte extrait dans un fichier.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "def main():\n",
    "    start_url = BASE_URL + START_PAGE\n",
    "    links = get_links_from_index(start_url)\n",
    "    \n",
    "    all_content = \"\"\n",
    "    for idx, link in enumerate(links):\n",
    "        print(f\"Scraping {idx+1}/{len(links)}: {link}\")\n",
    "        all_content += scrape_page(link)\n",
    "        time.sleep(1)  # Éviter de surcharger le serveur\n",
    "    \n",
    "    save_to_file(all_content)\n",
    "    print(\"Scraping terminé ! Fichier `docs_python.md` généré.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fonctionnal test code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_element(element,soup) -> str:\n",
    "    \"\"\"\n",
    "    Searches for the first occurrence of a specified HTML element in a BeautifulSoup object and returns its text.\n",
    "\n",
    "    Parameters:\n",
    "    - element (str): The tag name of the HTML element to search for (e.g., 'h1', 'div').\n",
    "    - soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text of the first occurrence of the specified element if found; otherwise, an empty string.\n",
    "    \"\"\"\n",
    "    result = soup.find(element)\n",
    "    if result:\n",
    "        return result.text\n",
    "    else:\n",
    "        print(f\"No element ${element} found.\")\n",
    "        return \"\"\n",
    "\n",
    "url = 'https://docs.python.org/3/howto/logging.html'\n",
    "response = requests.get(url)\n",
    "if (error := response.status_code) == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    raise f\"Status code error: {error}\"\n",
    "\n",
    "### define soup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "### get title\n",
    "title = get_html_element('h1',soup) # for front matter\n",
    "title_name = title.lower().replace(\" \",\"-\") # for filename\n",
    "\n",
    "### get subtitle\n",
    "subtitle = get_html_element('h2',soup) # for front matter\n",
    "\n",
    "### code blocks\n",
    "html_content = html_content.replace(\"<pre\", \"```<pre\")\n",
    "html_content = html_content.replace(\"</pre>\", \"</pre>```\")\n",
    "\n",
    "### text separators\n",
    "# Find all elements with role=\"separator\"\n",
    "separator_elements = soup.find_all(attrs={\"role\": \"separator\"})\n",
    "\n",
    "# replace with <hr> element, markdown recognizes this\n",
    "for element in separator_elements:\n",
    "    html_content = html_content.replace(str(element), \"<hr>\")\n",
    "\n",
    "html_converter = HTML2Text()\n",
    "html_converter.ignore_links = False\n",
    "markdown_content = html_converter.handle(html_content)\n",
    "\n",
    "### get formatted date\n",
    "today = datetime.now()\n",
    "formatted_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save file to _posts folder\n",
    "filename = f\"{formatted_date_str}-{title_name}.md\"\n",
    "\n",
    "with open(f\"{filename}\", 'w', encoding='utf-8') as file:\n",
    "    file.write(markdown_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
